{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome","text":"<p>This tutorial walks you through setting up a scalable, cloud-based data platform using:</p> <ul> <li>Snowflake for cloud data storage &amp; compute</li> <li>dbt Cloud for data transformation</li> <li>Fivetran for data ingestion</li> </ul> <p>Of course, these tools are far more capable than described above.</p> <p>You'll learn how to integrate these tools efficiently and provide you with the foundations to deliver high-quality analytics data.</p> <p>What You'll Achieve</p> <ul> <li>Create and configure a cloud-based data warehouse.</li> <li>Automate data ingestion and transformation.</li> <li>Set up CI/CD and version control for analytics.</li> <li>Implement data observability for quality assurance.</li> <li>Build dashboards and reports with structured insights.</li> </ul> <p>Browser </p> <p>This tutorial has been tested based on Google Chrome . It may be, that certain platforms do not work entirely flawless with other browsers.</p> <p>Get Started Here: Snowflake Setup \ud83d\ude80</p>"},{"location":"data-quality/","title":"Data Quality Testing in dbt","text":""},{"location":"data-quality/#introduction","title":"Introduction","text":"<p>Ensuring data integrity is critical for reliable analytics. dbt allows automated data testing to catch inconsistencies before they impact downstream reports. By implementing column-level tests, we can enforce business rules and improve trust in our data models.</p> <p>dbt provides built-in test types and supports custom tests through packages like dbt-utils and dbt-expectations.</p>"},{"location":"data-quality/#1-defining-data-quality-tests","title":"1. Defining Data Quality Tests","text":"<p>In dbt, tests are defined within the <code>.yml</code> files alongside the models (preferred) or schemas they validate.</p> <p>Example: Basic Tests on the <code>stg_orders</code> Model in the staging schema ./models/staging/staging.yml<pre><code>models:\n    - name: stg_orders\n      description: \"Staging model for orders\"\n      columns:\n        - name: order_id\n          description: \"Unique identifier for each order\"\n          tests:\n            - unique\n            - not_null\n        - name: order_total\n          description: \"Total value of the order\"\n          tests:\n            - not_null\n            - dbt_utils.accepted_range:\n                min_value: 0\n</code></pre></p> <p>Test Types</p> <ul> <li>unique: Ensures that order_id does not contain duplicates</li> <li>not_null: Ensures order_total and order_id have no missing values</li> <li>accepted_range: Ensures order_total is always \u2265 0 (i.e., no negative totals)</li> </ul>"},{"location":"data-quality/#2-add-dbt-expectations-for-advanced-testing","title":"2. Add dbt-expectations for advanced testing","text":"<p>Add package dbt-expectations to your packages.yml file</p> ./packages.yml<pre><code>packages:\n  - package: dbt-labs/dbt_utils\n    version: [\"&gt;=1.0.0\", \"&lt;2.0.0\"]\n  - package: calogica/dbt_expectations\n    version: [\"&gt;=0.10.1\", \"&lt;0.11.0\"]\n</code></pre> <p>Run dbt deps to install the package</p> <pre><code>dbt deps\n</code></pre> <p>See all the tests available in the dbt-expectations package here.</p>"},{"location":"data-quality/#3-add-data-quality-tests","title":"3. Add data quality tests","text":"<p>Add data quality tests to your dbt project in the source.yml file</p> ./models/staging/source.yml<pre><code>version: 2\n\nsources:\n  - name: jaffle_shop\n    database: PSA\n    schema: JAFFLE_SHOP\n    description: Mock data\n    tables:\n      - name: raw_customers\n        description: One record per person who has purchased one or more items\n        columns:\n          - name: ID\n            tests:\n              - unique  # Ensures no duplicate Order IDs\n              - not_null  # Ensures no NULL values        \n      - name: raw_orders\n        description: One record per order (consisting of one or more order items)\n        columns:\n          - name: ID\n            tests:\n              - unique  # Ensures no duplicate Order IDs\n              - not_null  # Ensures no NULL values\n          - name: CUSTOMER\n            tests:\n              - not_null  # Ensures all orders are linked to a customer\n              - relationships: # Ensures every CUSTOMER(_ID) in raw_orders exists in raw_customers as ID\n                  to: source('jaffle_shop', 'raw_customers')\n                  field: ID\n          - name: ORDER_TOTAL\n            tests:\n              - not_null\n              - dbt_expectations.expect_column_values_to_be_between:\n                  min_value: 0  # Allows 0 and positive values\n                  severity: warn  # Logs a warning instead of failing the run\n      - name: raw_items\n        description: Items included in an order\n      - name: raw_stores\n        description: All stores from jaffle shop\n      - name: raw_products\n        description: One record per SKU for items sold in stores\n      - name: raw_supplies\n        description: One record per supply per SKU of items sold in stores\n</code></pre>"},{"location":"data-quality/#4-running-tests","title":"4. Running Tests","text":"<p>Running dbt tests is as simple as adding the below code to the command line and run it with <code>Cmd + Enter</code> on Mac or <code>Ctrl + Enter</code> in Windows. Execute the command below:</p>"},{"location":"data-quality/#dbt-test","title":"<pre><code>dbt test\n</code></pre>","text":"<p>This command will:</p> <ul> <li>Validate all tests in the project.</li> <li>Identify any failing records.</li> <li>Provide a summary of passed/failed tests.</li> </ul> <p>Example output:</p> Timestamp Status 16:12:45 1 of 3 PASS unique test on stg_orders.order_id 16:12:45 2 of 3 PASS not_null test on stg_orders.order_total 16:12:45 3 of 3 FAIL accepted_range test on stg_orders.order_total <p>\u274c A failure on accepted_range means there are negative order totals that need fixing.</p> <p>In our case, you should see the following results:</p> <p></p> <p>Great! All tests passed!\ud83c\udf89</p> <p>Commit the changes to your repository. Add a commit message <code>Add data quality tests</code>.</p> <p>Merge the branch into main.</p>"},{"location":"data-quality/#5-advanced-granular-test-configuration","title":"5. Advanced Granular Test Configuration","text":"<p>dbt allows defining granular conditions within test configurations. This ensures flexibility and targeted validation for business rules. If you like, you can also add this logic in your code in a new file <code>./marts/staging/staging.yml</code>.</p> <p>Example: Conditional Severity for order_total</p> Granular Testing<pre><code>models:\n  - name: stg_orders\n    columns:\n      - name: order_total\n        tests:\n          - not_null:\n              config:\n                severity: error\n                error_if: \"&gt;50\"\n                warn_if: \"&gt;0\"\n                where: \"status IN ('completed', 'shipped')\"\n</code></pre> <p>Dynamic thresholds:</p> <ul> <li>error_if: \"&gt;50\" \u2192 Fails if order_total exceeds 50</li> <li>warn_if: \"&gt;0\" \u2192 Raises a warning if order_total is greater than 0</li> <li>where: \"status IN ('completed', 'shipped')\" \u2192 Applies only to completed &amp; shipped orders</li> </ul>"},{"location":"data-quality/#6-tip-for-handling-test-failures","title":"6. Tip for Handling Test Failures","text":"<p>When a test fails, dbt provides detailed logs with failing records. You can let dbt add this information into snowflake to identify failed records. More information here.</p> <p>This helps teams quickly identify, debug, and fix data issues.</p> <p>\ud83c\udf89 Next Steps</p> <p>By integrating dbt\u2019s testing capabilities, we can automate data validation and prevent bad data from entering downstream reports.</p> <p>\ud83d\udd17 Continue to: \ud83d\udd17 Continue to: dbt Documentation &amp; Deployment</p>"},{"location":"data-visualization/","title":"Data Visualization in Snowflake","text":""},{"location":"data-visualization/#introduction","title":"Introduction","text":"<p>Once our data has been ingested, transformed, and validated, the next step is visualizing it to enable better business insights. Snowflake Dashboards provide a built-in, out-of-the-box visualization tool, allowing users to explore data without third-party BI tools.  </p> <p>By the end of this section, you will:</p> <ul> <li>Have a fully built dashboard in Snowflake </li> <li>Use interactive charts for business analysis  </li> </ul>"},{"location":"data-visualization/#1-creating-a-new-dashboard-in-snowflake","title":"1. Creating a New Dashboard in Snowflake","text":"<p>To start visualizing your data:</p> <ol> <li>Go to the Snowflake UI, navigate to the top left and +Create &gt; Dashboard. Give it a name like <code>Sales Performance</code>. Create Dashboard.</li> </ol> <p></p> <ol> <li> <p>Click on \"New Tile and select \"Create New Dashboard\".</p> <p></p> <p>and select \"From SQL Worksheet\".</p> <p></p> </li> <li> <p>Set the Context (<code>ANALYTICS.DM</code>).</p> </li> </ol> <p></p> <p>Proceed to the next step to create a tile (chart) for the dashboard.</p>"},{"location":"data-visualization/#2-creating-a-tile-chart-for-the-dashboard","title":"2. Creating a Tile (Chart) for the Dashboard","text":"<ol> <li> <p>Enter your SQL query in the SQL Worksheet and rename the sheet at the top in the middle.</p> <p></p> </li> </ol> <p>KPI Total Revenue<pre><code>SELECT\n    SUM(ORDER_TOTAL) AS total_revenue\nFROM orders;\n</code></pre>  or if you want to be more explicit:</p> KPI Total Revenue<pre><code>SELECT \n    SUM(ORDER_TOTAL) AS TOTAL_REVENUE\nFROM ANALYTICS.DM.ORDERS;\n</code></pre> Additional Examples <p>Please, adapt <code>FROM ANALYTICS.DM.&lt;Table&gt;</code> to your schema.</p> <p>KPIs Total Customers<pre><code>SELECT \n    COUNT(DISTINCT CUSTOMER_ID) AS TOTAL_CUSTOMERS\nFROM ANALYTICS.DM.ORDERS;\n</code></pre></p> Total Orders<pre><code>SELECT \n    COUNT(*) AS TOTAL_ORDERS\nFROM ANALYTICS.DM.ORDERS;\n</code></pre> Average Order Value (AOV)<pre><code>SELECT \n    SUM(ORDER_TOTAL) / NULLIF(COUNT(*), 0) AS AVG_ORDER_VALUE\nFROM ANALYTICS.DM.ORDERS;\n</code></pre> <p>Line Charts Revenue Trend (Daily)<pre><code>SELECT \n    DATE_TRUNC('DAY', ORDERED_AT) AS ORDER_DATE, \n    SUM(ORDER_TOTAL) AS DAILY_REVENUE\nFROM ANALYTICS.DM.ORDERS\nGROUP BY ORDER_DATE\nORDER BY ORDER_DATE;\n</code></pre></p> Orders Trend (Daily)<pre><code>SELECT \n    DATE_TRUNC('DAY', ORDERED_AT) AS ORDER_DATE, \n    COUNT(*) AS DAILY_ORDERS\nFROM ANALYTICS.DM.ORDERS\nGROUP BY ORDER_DATE\nORDER BY ORDER_DATE;\n</code></pre> <p>Bar Charts</p> Revenue by Product Category (Food vs. Drink)<pre><code>SELECT \n    CASE \n        WHEN IS_FOOD_ITEM = 1 THEN 'Food' \n        WHEN IS_DRINK_ITEM = 1 THEN 'Drink' \n        ELSE 'Other' \n    END AS CATEGORY,\n    SUM(PRODUCT_PRICE) AS TOTAL_REVENUE\nFROM ANALYTICS.DM.ORDER_ITEMS\nGROUP BY CATEGORY\nORDER BY TOTAL_REVENUE DESC;\n</code></pre> Most Popular Products (Top 10)<pre><code>SELECT \n    PRODUCT_ID, \n    COUNT(*) AS TOTAL_SALES, \n    SUM(PRODUCT_PRICE) AS TOTAL_REVENUE\nFROM ANALYTICS.DM.ORDER_ITEMS\nGROUP BY PRODUCT_ID\nORDER BY TOTAL_SALES DESC\nLIMIT 10;\n</code></pre> Gross Profit by Order (Top 10)<pre><code>SELECT \n    ORDER_ID, \n    SUM(PRODUCT_PRICE - SUPPLY_COST) AS GROSS_PROFIT\nFROM ANALYTICS.DM.ORDER_ITEMS\nGROUP BY ORDER_ID\nORDER BY GROSS_PROFIT DESC\nLIMIT 10;\n</code></pre> Profit Margin Products (Top 10)<pre><code>SELECT \n    PRODUCT_ID,\n    SUM(PRODUCT_PRICE - SUPPLY_COST) * 100.0 / SUM(PRODUCT_PRICE) AS PROFIT_MARGIN\nFROM ANALYTICS.DM.ORDER_ITEMS\nWHERE :daterange = ORDERED_AT\nGROUP BY PRODUCT_ID\nORDER BY PROFIT_MARGIN desc\nLIMIT 10;\n</code></pre> <ol> <li> <p>Click Run to preview the results.</p> <p>Run the query by pressing on Win <code>CTRL + Enter</code>, Mac <code>CMD + Enter</code> or by clicking the Run button in the top right.</p> </li> </ol>"},{"location":"data-visualization/#3-choosing-a-chart-type","title":"3. Choosing a Chart Type","text":"<ol> <li>Click on the \"Chart\" tab to visualize the data.</li> <li>Select a Chart type:</li> <li>Line Chart \u2013 Show trends over time.</li> <li>Bar Chart \u2013 Compare total orders by status.</li> <li>Scatter Plot \u2013 Analyze the relationship between order total and quantity.</li> <li>Heat Grid \u2013 Visualize order distribution.</li> <li> <p>Scorecard \u2013 Display a single KPI value.</p> <p></p> </li> <li> <p>Add the Tile to your Dashboard by clicking on \"Return to \". <p></p> <p>Repeat this process by clicking on <code>+</code> in the top left to add more tiles to your dashboard.</p>"},{"location":"data-visualization/#4-enhancing-the-dashboard-with-filters","title":"4. Enhancing the Dashboard with Filters","text":"<ol> <li> <p>Select a Tile and Edit Query.</p> <p>If you select the filter icon in the top left, </p> <p></p> <p>you can see which filters are available by default, </p> <p></p> <p>along with the option to add your custom filters.</p> </li> <li> <p>Add a Filter to the query.</p> <p></p> Total Revenue<pre><code>SELECT\n    SUM(ORDER_TOTAL) AS total_revenue\nFROM ANALYTICS.DM.ORDERS\nWHERE :daterange = ordered_at;\n</code></pre> </li> </ol> Additional Examples with filter <p>Add a date field available in the table to the query.</p> <p>KPIs Total Revenue<pre><code>SELECT \n    SUM(ORDER_TOTAL) AS TOTAL_REVENUE\nFROM ANALYTICS.DM.ORDERS\nWHERE :daterange = ORDERED_AT;\n</code></pre> Total Customers<pre><code>SELECT \n    COUNT(DISTINCT CUSTOMER_ID) AS TOTAL_CUSTOMERS\nFROM ANALYTICS.DM.ORDERS;\n</code></pre> Total Orders<pre><code>SELECT \n    COUNT(*) AS TOTAL_ORDERS\nFROM ANALYTICS.DM.ORDERS\nWHERE :daterange = ORDERED_AT;\n</code></pre></p> Average Order Value (AOV)<pre><code>SELECT \n    SUM(ORDER_TOTAL) / NULLIF(COUNT(*), 0) AS AVG_ORDER_VALUE\nFROM ANALYTICS.DM.ORDERS\nWHERE :daterange = ORDERED_AT;\n</code></pre> <p>Line Charts Revenue Trend (Daily)<pre><code>SELECT \n    DATE_TRUNC('DAY', ORDERED_AT) AS ORDER_DATE, \n    SUM(ORDER_TOTAL) AS DAILY_REVENUE\nFROM ANALYTICS.DM.ORDERS\nWHERE :daterange = ORDERED_AT\nGROUP BY ORDER_DATE\nORDER BY ORDER_DATE;\n</code></pre></p> Orders Trend (Daily)<pre><code>SELECT \n    DATE_TRUNC('DAY', ORDERED_AT) AS ORDER_DATE, \n    COUNT(*) AS DAILY_ORDERS\nFROM ANALYTICS.DM.ORDERS\nWHERE :daterange = ORDERED_AT\nGROUP BY ORDER_DATE\nORDER BY ORDER_DATE;\n</code></pre> <p>Bar Charts</p> Revenue by Product Category (Food vs. Drink)<pre><code>SELECT \n    CASE \n        WHEN IS_FOOD_ITEM = 1 THEN 'Food' \n        WHEN IS_DRINK_ITEM = 1 THEN 'Drink' \n        ELSE 'Other' \n    END AS CATEGORY,\n    SUM(PRODUCT_PRICE) AS TOTAL_REVENUE\nFROM ANALYTICS.DM.ORDER_ITEMS\nWHERE :daterange = ORDERED_AT\nGROUP BY CATEGORY\nORDER BY TOTAL_REVENUE DESC;\n</code></pre> Most Popular Products (Top 10)<pre><code>SELECT \n    PRODUCT_ID, \n    COUNT(*) AS TOTAL_SALES, \n    SUM(PRODUCT_PRICE) AS TOTAL_REVENUE\nFROM ANALYTICS.DM.ORDER_ITEMS\nWHERE :daterange = ORDERED_AT\nGROUP BY PRODUCT_ID\nORDER BY TOTAL_SALES DESC\nLIMIT 10;\n</code></pre> Gross Profit by Order (Top 10)<pre><code>SELECT \n    ORDER_ID, \n    SUM(PRODUCT_PRICE - SUPPLY_COST) AS GROSS_PROFIT\nFROM ANALYTICS.DM.ORDER_ITEMS\nWHERE :daterange = ORDERED_AT\nGROUP BY ORDER_ID\nORDER BY GROSS_PROFIT DESC;\n</code></pre> Profit Margin Products (Top 10)<pre><code>SELECT \n    PRODUCT_ID,\n    SUM(PRODUCT_PRICE - SUPPLY_COST) * 100.0 / SUM(PRODUCT_PRICE) AS PROFIT_MARGIN\nFROM ANALYTICS.DM.ORDER_ITEMS\nWHERE :daterange = ORDERED_AT\nGROUP BY PRODUCT_ID\nORDER BY PROFIT_MARGIN desc\nLIMIT 10;\n</code></pre> <ol> <li> <p>Return to your Dashboard and repeat that process for other tiles to add filters.</p> <p>Dashboard Filters</p> <p>Filters can be applied to multiple tiles, allowing users to interactively explore data. You can control the filter at the top left.</p> <p></p> </li> <li> <p>Customize the layout by dragging and resizing tiles for better clarity. Congratulations on building your first Snowflake dashboard!</p> <p></p> </li> </ol> <p>\u2705 Your dashboard is now live and interactive! Users can filter, explore and share insights directly within Snowflake.</p>"},{"location":"data-visualization/#this-is-the-end-of-the-hands-on-lab","title":"\ud83c\udf89 This is the end of the Hands-On Lab. \ud83c\udf89","text":"<p>\ud83d\udd17 For more References, go to: Further Resources</p>"},{"location":"dbt-deployment/","title":"dbt Deployment &amp; Documentation","text":""},{"location":"dbt-deployment/#introduction","title":"Introduction","text":"<p>Deploying dbt models effectively ensures that transformations run on a scheduled basis, providing fresh and reliable data to downstream analytics.</p> <p>Additionally, documentation serves as a centralized source of truth, helping teams understand data lineage, model definitions, and test coverage.</p> <p>At the moment, the Explore section in dbt Cloud is empty. Let's deploy our models and generate documentation to populate it.</p> <p>Merge to main</p> <p>Make sure, you merged all your commits/changes to the main branch! <code>Develop</code> &gt; <code>Cloud IDE</code> &gt; It should say <code>Create branch</code>.</p>"},{"location":"dbt-deployment/#_1","title":"6. Documentation & Deployment","text":""},{"location":"dbt-deployment/#1-setting-up-a-production-environment","title":"1. Setting Up a Production Environment","text":"<p>A production environment ensures that only tested, validated models are exposed to analysts and end consumers (Tableau, PowerBI, Excel, etc.).</p>"},{"location":"dbt-deployment/#steps-to-set-up-a-production-environment","title":"Steps to Set Up a Production Environment","text":"<ol> <li>Go to dbt Cloud \u2192 Click Deploy &gt; Environments.</li> <li>Create a New Environment:</li> <li>Name: <code>PRD</code></li> <li>Deployment type: <code>Production</code></li> <li>Connection: <code>Snowflake_HWZ</code></li> <li>Deployment credentials: Use the Snowflake credentials.</li> <li> <p>Schema: <code>default</code></p> <p></p> <p></p> </li> <li> <p>Test the Connection to ensure a successful setup.</p> <p></p> </li> <li> <p>Save the environment.</p> </li> </ol>"},{"location":"dbt-deployment/#2-setting-up-a-production-deploy-job","title":"2. Setting Up a Production Deploy Job","text":"<p>Go to the bottom right <code>+ Create job</code></p> <p></p> <p>Select <code>Deploy job</code></p> <p></p> <p>Add the following details:</p> <ul> <li>Job name: <code>PRD</code></li> <li>Environment: <code>PRD</code></li> <li> <p>Generate docs on run: [x]</p> <p></p> </li> <li> <p>Target name: <code>PRD</code></p> </li> <li> <p>Threads: <code>16</code></p> <p></p> </li> </ul> <p>Save the job.</p> <p>Run now to deploy the models to the production environment.</p> <p> </p> <p>Success! The models are now deployed to the production environment and the documentation is generated.</p> <p></p> <p>Go to the Explore section to view the documentation.</p> <p></p> <p>Since we haven\u2019t added many <code>&lt;schema&gt;.yml</code> files, tests, or other features yet, the documentation is not very detailed. But this will get richer as we add more features to our dbt project.</p>"},{"location":"dbt-deployment/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>Your dbt project is now automated, tested, and documented! Proceed to building dashboards with your transformed data.</p> <p>\ud83d\udd17 Continue to: Building Dashboards with Snowflake</p>"},{"location":"dbt-setup/","title":"dbt Cloud Setup","text":"<p>MFA required</p> <p>With the second sign in, dbt cloud it asking for multi factor authentication.</p>"},{"location":"dbt-setup/#1-create-a-dbt-cloud-account","title":"1. Create a dbt Cloud Account","text":"<ol> <li> <p>To start using dbt Cloud, sign up for an account on the dbt Cloud website. Start free trial.</p> <p>Note down dbt credentials</p> <p>Add username and password to your text file.</p> <p></p> </li> <li> <p>You will receive an email for the verification process.</p> <p></p> <p>Click on the Verify your email address to confirm your account.</p> <p></p> </li> <li> <p>Account setup complete! You can dismiss any pop-ups that might appear.</p> <p></p> </li> </ol>"},{"location":"dbt-setup/#2-complete-the-project-setup","title":"2. Complete the project setup","text":"<p>dbt Cloud will per default set up a new project called <code>ANALYTICS</code>, which we will leverage for our lab.</p> <ol> <li> <p>Select dropdown for \"Connection\". Add new connection.</p> <p></p> </li> <li> <p>Select your data warehouse <code>Snowflake</code>.</p> <p></p> </li> <li> <p>Fill out the form .</p> </li> </ol> <p>Make sure the full account locator is correct</p> <p>Snowflake's account locators might be different from the previousl versions. Make sure to check your snowflake account locator has no AWS_ in the middle. If it does, remove it.</p> <p>Ideally, it would look like this: kg10297<sup>1</sup>.eu_central_2.aws<sup>2</sup></p> <p>Not kg10297.aws_eu_central_2.aws</p> <ul> <li>Connection name: <code>Snowflake_HWZ</code></li> <li>Account: <code>Full Account Locator</code> from your notes in the Snowflake section.</li> <li>Database (Target in Snowflake): <code>Analytics</code></li> <li> <p>Warehouse: <code>Transformer</code></p> <p></p> <p></p> </li> </ul>"},{"location":"dbt-setup/#3-connect-dbt-cloud-to-snowflake-with-adding-your-developer-credentials","title":"3. Connect dbt Cloud to Snowflake with adding your developer credentials","text":"<p>Go back to the Dashboard on the top left corner to configure your development environment.</p> <p>To enable dbt transformations, you need to add the development credentials to your connection for dbt Cloud to your Snowflake instance:</p> <ol> <li> <p>Choose Snowflake_HWZ as your Connection.</p> <p></p> </li> <li> <p>Enter the required Snowflake credentials:</p> <ul> <li>Username from your snowflake account</li> <li>Password from your snowflake account</li> <li>Schema (leave the default: dbt_) <li>Target name (leave: <code>default</code>)</li> <li>Threads: <code>16</code> <sup>3</sup></li> <p></p> <p>Test Connection to verify the connection between dbt Cloud and Snowflake works.</p> <li> <p>Save Development credentials if successful.</p> <p> </p> </li>"},{"location":"dbt-setup/#4-setup-the-project-repository-to-complete-the-project-setup","title":"4. Setup the Project Repository to complete the Project Setup","text":"<p>For simplicity, we are going to use the dbt Cloud IDE to manage our project repository.</p> <ol> <li> <p>Select Managed and add a name for your repository. <code>analytic-hwz</code> for example.</p> <p></p> <p></p> </li> <li> <p>Success - Let's go to Start developing in the IDE</p> <p></p> <p></p> </li> </ol>"},{"location":"dbt-setup/#5-initialize-your-analytics-project","title":"5. Initialize your Analytics Project","text":"<p>The initialization creates the following project base structure:</p> <pre><code>  \ud83d\udce6 your_dbt_project\n  \u251c\u2500\u2500 \ud83d\udcc2 analyses\n  \u251c\u2500\u2500 \ud83d\udcc2 macros\n  \u251c\u2500\u2500 \ud83d\udcc2 models\n  \u251c\u2500\u2500 \ud83d\udcc2 seeds\n  \u251c\u2500\u2500 \ud83d\udcc2 snapshots\n  \u251c\u2500\u2500 \ud83d\udcc2 target\n  \u251c\u2500\u2500 \ud83d\udcc2 tests\n  \u251c\u2500\u2500 \ud83d\udcc4 .gitignore\n  \u251c\u2500\u2500 \ud83d\udcc4 README\n  \u2514\u2500\u2500 \ud83d\udcc4 dbt_project\n</code></pre> <ol> <li> <p>Ignore the upcoming pop-ups.</p> <p></p> </li> <li> <p>Initialize dbt project by clicking on the button in the top left.</p> <p></p> </li> <li> <p>Initialization comes with an example model, which we do not care about. Delete the example model <code>./models/example</code>.</p> <p></p> <p></p> </li> <li> <p>Commit and sync the changes.</p> <p></p> <p>Add a message like <code>Initial commit</code> and hit Commit changes.</p> <p></p> </li> <li> <p>Success - your dbt project is now ready for development.</p> <p></p> </li> </ol>"},{"location":"dbt-setup/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>Now that dbt Cloud is configured, proceed to setting up transformations and version control.</p> <p>\ud83d\udd17 Continue to: Data loading with Fivetran</p> <ol> <li> <p>kg10297 is the account locator\u00a0\u21a9</p> </li> <li> <p>eu_central_2 is the region\u00a0\u21a9</p> </li> <li> <p>Not essential, but good practice as we do not want to artificially limit Snowflakes capabilities. The smallest warehouse starts with 8 threads.\u00a0\u21a9</p> </li> </ol>"},{"location":"dbt-transformation/","title":"dbt Transformation","text":""},{"location":"dbt-transformation/#introduction","title":"Introduction","text":"<p>In today\u2019s data-driven world, organizations generate vast amounts of information from various sources\u2014transactional systems, IoT devices, third-party APIs, and more. However, raw data alone is not enough. Without proper curation, governance, and accessibility, businesses struggle with inconsistent reporting, poor data quality, and unscalable analytics workflows.</p> <p>This is where data warehousing plays a crucial role. A modern data warehouse, built on platforms like Snowflake, acts as a central repository for structured, semi-structured and unstructured data, ensuring consistency, security, and efficiency/automation in analytics and decision-making.</p>"},{"location":"dbt-transformation/#key-benefits-of-a-data-warehouse","title":"Key Benefits of a Data Warehouse","text":"<p>\u2705 Data Curation &amp; Standardization \u2013 Transform raw data into well-modeled, analytics-ready datasets \u2705 Automation &amp; Orchestration \u2013 Reduce manual data processing with scheduled transformations \u2705 Data Quality &amp; Monitoring \u2013 Implement tests to detect anomalies and inconsistencies early \u2705 Centralized Repository \u2013 Provide a single source of truth across departments \u2705 Scalability &amp; Performance \u2013 Leverage cloud-based architectures to handle growing data volumes  </p> <p>Despite these advantages, the landscape of data architecture has seen various trends\u2014some short-lived, others transformative.</p> <p></p> <p>\u201cData Warehousing is dead!\u201d they said\u2014until Data Lakes became ungoverned messes, Lakehouses reinvented warehouses with Parquet, and AI-driven modeling looked suspiciously like moving targets. Meanwhile, dbt materialized, Snowflake auto-scaled, and a CFO refreshed their Tableau dashboard. Nice try, kid! </p> <p>Let's get back to work:</p> <p>This part will cover, which elements of a data warehouse need to be addressed. We will not fully implement all documentation, tests and leave out the intermediate data warehouse layer (e.g. DataVault - too complex for this setting). </p>"},{"location":"dbt-transformation/#1-prepare-project-setup-in-dbt","title":"1. Prepare Project Setup in dbt","text":"<p>Before starting with dbt transformations, we need to add some more configurations to the project.</p> <ol> <li> <p>Go to Develop &gt; Cloud IDE.</p> </li> <li> <p>Create a branch named <code>feature/&lt;your-name&gt;-base-setup-jaffle-shop</code> for your changes.</p> <p></p> <p></p> </li> <li> <p>Modify <code>dbt_project.yml</code>:</p> dbt_project.yml<pre><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: \"jaffle_shop\"\nversion: \"1.0.0\"\nconfig-version: 2\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: \"snowflake\"\nrequire-dbt-version: \"&gt;=1.6.0rc2\"\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"jaffle-data\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\ntarget-path: \"target\" # directory which will store compiled SQL files\nclean-targets: # directories to be removed by `dbt clean`\n- \"target\"\n- \"dbt_packages\"\n\nvars:\n    ignore_default_schema: false\n    truncate_timespan_to: \"{{ current_timestamp() }}\"\n    \"dbt_date:time_zone\": \"Switzerland/Zurich\"\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/ directory\n# as tables. These settings can be overridden in the individual model files\n# using the `{{ config(...) }}` macro.\n\nmodels:\n    jaffle_shop:\n        staging:\n            +materialized: view\n            +schema: stg\n        marts:\n            +materialized: table\n            +schema: dm\n</code></pre> </li> <li> <p>Add under macros the following code into the file ./macros/generate_schema_name.sql</p> <p>./macros/generate_schema_name.sql<pre><code>{% macro generate_schema_name(custom_schema_name, node) -%}\n    {%- set default_schema = target.schema -%}\n    {%- if custom_schema_name is none or target.name == 'PRD' -%}\n        {{ custom_schema_name | trim }}\n    {%- else -%}\n        {%- if var('ignore_default_schema')==true -%}\n        {{ custom_schema_name | trim }}\n        {%- else -%}\n        {{ default_schema }}_{{ custom_schema_name | trim }}\n        {%- endif -%}\n    {%- endif -%}\n{%- endmacro %}\n</code></pre> This macro will be used to generate the schema name for the tables and views that will be created in Snowflake and makes it more readable.</p> </li> <li> <p>Commit changes:</p> <ul> <li>Use a message like <code>dbt project setup</code>.</li> <li>Click Commit Changes.</li> </ul> </li> </ol>"},{"location":"dbt-transformation/#2-define-data-sources","title":"2. Define Data Sources","text":"<p>Now, let's define our data sources within dbt. This helps dbt to understand, which data tables from where (database, schema) we will be using.</p> <ol> <li>Create a new folder: <pre><code>./models/staging\n</code></pre></li> <li>Create a new source definition file:    <pre><code>./models/staging/source.yml\n</code></pre></li> <li> <p>Add the following content (from the provided source file):</p> <pre><code>version: 2\n\nsources:\n  - name: jaffle_shop\n    database: PSA\n    schema: JAFFLE_SHOP\n    description: Mock data\n    tables:\n    - name: raw_customers\n      description: One record per person who has purchased one or more items\n    - name: raw_orders\n      description: One record per order (consisting of one or more order items)\n    - name: raw_items\n      description: Items included in an order\n    - name: raw_stores\n      description: All stores from jaffle shop\n    - name: raw_products\n      description: One record per SKU for items sold in stores\n    - name: raw_supplies\n      description: One record per supply per SKU of items sold in stores\n</code></pre> </li> <li> <p>Commit the changes. Add a message like <code>dbt source definition</code>.</p> </li> </ol> Bonus: Automate this step in the future <p>In dbt, there is a concept of packages that allows you to use macros for automation. dbt can generate the above source.yml file.</p> <ol> <li> <p>Add in the root directory (upper most level) a file called <code>packages.yml</code>.</p> </li> <li> <p>Insert the below code snippet to define the required codegen (code generator) package. packages.yml<pre><code>packages:\n- package: dbt-labs/codegen\n  version: 0.13.1\n</code></pre></p> </li> <li> <p>Run <code>dbt deps</code> in the command line, to install the package we just defined.</p> </li> <li> <p>Run the generate_source macro in the command line bar on the bottom to make it create the source.yml content.</p> <p><pre><code>dbt run-operation generate_source --args '{\n    \"database_name\": \"PSA\",\n    \"schema_name\": \"JAFFLE_SHOP\",\n    \"generate_columns\": true,\n    \"include_descriptions\": true,\n    \"include_data_types\": true\n    }'\n</code></pre> For more details about codegen, refer to the documentation.</p> <p></p> </li> <li> <p>After the successful run, we will copy the output into a new source.yml file, in the folder staging. If it does not yet exist.</p> <p></p> <p></p> <p>Open the <code>System Logs</code>, select <code>Details</code> and scroll down until you find the .yml section.</p> <p></p> <p></p> <p>Copy the snippet and paste it in a new file <code>./models/staging/source.yml</code>.</p> </li> </ol>"},{"location":"dbt-transformation/#3-build-staging-models","title":"3. Build Staging Models","text":"<p>Staging models are the initial data warehouse layer to transform raw data into more structured &amp; clean formats.</p> <ol> <li> <p>Create SQL files in the <code>staging</code> folder:    <pre><code> ./models/staging/stg_customers.sql\n ./models/staging/stg_locations.sql\n ./models/staging/stg_order_items.sql\n ./models/staging/stg_orders.sql\n ./models/staging/stg_products.sql\n ./models/staging/stg_supplies.sql\n</code></pre></p> </li> <li> <p>Define a basic transformation:</p> <p>Add the following code to your staging folder files:</p> ./staging/stg_customers.sql <pre><code>with source as (\n\n    select * from {{ source('jaffle_shop', 'raw_customers') }}\n\n),\n\nrenamed as (\n\n    select\n\n        ----------  ids\n        id as customer_id,\n\n        ---------- text\n        name as customer_name\n\n    from source\n\n)\n\nselect * from renamed\n</code></pre> ./staging/stg_locations.sql <pre><code>with\n\nsource as (\n\n    select * from {{ source('jaffle_shop', 'raw_stores') }}\n\n    -- if you generate a larger dataset, you can limit the timespan to the current time with the following line\n    -- where ordered_at &lt;= {{ var('truncate_timespan_to') }}\n),\n\nrenamed as (\n\n    select\n\n        ----------  ids\n        id as location_id,\n\n        ---------- text\n        name as location_name,\n\n        ---------- numerics\n        tax_rate,\n\n        ---------- timestamps\n        {{dbt.date_trunc('day', 'opened_at')}} as opened_at\n\n    from source\n\n)\n\nselect * from renamed\n</code></pre> ./staging/stg_order_items.sql <pre><code>with\n\nsource as (\n\n    select * from {{ source('jaffle_shop', 'raw_items') }}\n\n),\n\nrenamed as (\n\n    select\n\n        ----------  ids\n        id as order_item_id,\n        order_id,\n        sku as product_id\n\n    from source\n\n)\n\nselect * from renamed\n</code></pre> ./staging/stg_orders.sql <pre><code>with\n\nsource as (\n\n    select * from {{ source('jaffle_shop', 'raw_orders') }}\n    -- if you generate a larger dataset, you can limit the timespan to the current time with the following line\n    -- where ordered_at &lt;= {{ var('truncate_timespan_to') }}\n\n),\n\nrenamed as (\n\n    select\n\n        ----------  ids\n        id as order_id,\n        store_id as location_id,\n        customer as customer_id,\n\n        ---------- numerics\n        (order_total / 100.0) as order_total,\n        (tax_paid / 100.0) as tax_paid,\n\n        ---------- timestamps\n        {{dbt.date_trunc('day','ordered_at')}} as ordered_at\n\n    from source\n\n)\n\nselect * from renamed\n</code></pre> ./staging/stg_products.sql <pre><code>with\n\nsource as (\n\n    select * from {{ source('jaffle_shop', 'raw_products') }}\n\n),\n\nrenamed as (\n\n    select\n\n        ----------  ids\n        sku as product_id,\n\n        ---------- text\n        name as product_name,\n        type as product_type,\n        description as product_description,\n\n\n        ---------- numerics\n        (price / 100.0) as product_price,\n\n        ---------- booleans\n        case\n            when type = 'jaffle' then 1\n            else 0\n        end as is_food_item,\n\n        case\n            when type = 'beverage' then 1\n            else 0\n        end as is_drink_item\n\n    from source\n\n)\n\nselect * from renamed\n</code></pre> ./staging/stg_supplies.sql <pre><code>with\n\nsource as (\n\n    select * from {{ source('jaffle_shop', 'raw_supplies') }}\n\n),\n\nrenamed as (\n\n    select\n\n        ----------  ids\n        {{ dbt_utils.generate_surrogate_key(['id', 'sku']) }} as supply_uuid,\n        id as supply_id,\n        sku as product_id,\n\n        ---------- text\n        name as supply_name,\n\n        ---------- numerics\n        (cost / 100.0) as supply_cost,\n\n        ---------- booleans\n        perishable as is_perishable_supply\n\n    from source\n\n)\n\nselect * from renamed\n</code></pre> </li> <li> <p>Commit the changes. Use a message like <code>add dbt staging models</code>.</p> </li> </ol>"},{"location":"dbt-transformation/#4-add-packagesyml-for-dbt-utils","title":"4. Add <code>packages.yml</code> for dbt Utils","text":"<ol> <li>Create a <code>packages.yml</code> file in the root directory.</li> <li> <p>Add dbt packages for utilities:</p> <pre><code>packages:\n    - package: dbt-labs/dbt_utils\n      version: [\"&gt;=1.0.0\", \"&lt;2.0.0\"]\n</code></pre> </li> <li> <p>Run dependencies:</p> <p>Go to the terminal </p> <p></p> <p>and run the following command:</p> <p><pre><code>dbt deps\n</code></pre> Expect the following result:</p> <p></p> </li> </ol> <p>What does <code>dbt deps</code> do?</p> <p>The <code>dbt deps</code> command installs all dbt packages listed in your <code>packages.yml</code> file.  </p> <ul> <li>It fetches dependencies from the dbt Hub or a private repository.</li> <li>It installs them into the <code>dbt_packages/</code> directory.</li> <li>It ensures all packages are the correct version as defined in your configuration.</li> </ul> <p>Running this command is essential before executing models that rely on dbt-utils or other packages.</p>"},{"location":"dbt-transformation/#5-first-build-process","title":"5. First Build Process","text":"<ol> <li> <p>Run the first dbt build:    <pre><code>dbt build\n</code></pre></p> </li> <li> <p>This command will:</p> <ul> <li>Execute models (tables/views).</li> <li>Run tests and snapshots.</li> </ul> </li> <li> <p>Expected output:</p> <ul> <li>Staging models materialized as views in Snowflake.</li> <li>Lineage graphs showing dependencies.</li> </ul> <p></p> <p></p> </li> <li> <p>Check the Snowflake database:</p> <p>Verify that the staging models are created in the <code>ANALYTICS</code> database. Schema name is <code>DBT_&lt;Your-NAME&gt;_STG</code>.</p> <p></p> </li> <li> <p>Commit the changes. Use a message like <code>Staging base model complete and dbt build successful</code>.</p> </li> </ol>"},{"location":"dbt-transformation/#6-create-data-marts","title":"6. Create Data Marts","text":"<p>To provide structured data for analysis:</p> <ol> <li>Create a new folder:    <pre><code>./models/marts\n</code></pre></li> <li>Create three models:    <pre><code>./models/marts/customers.sql\n./models/marts/orders.sql\n./models/marts/order_items.sql\n</code></pre></li> <li> <p>Add the following models to the marts folder:</p> ./marts/customers.sql <pre><code>with\n\ncustomers as (\n\n    select * from {{ ref('stg_customers') }}\n\n),\n\norders_table as (\n\n    select * from {{ ref('orders') }}\n\n),\n\norder_items_table as (\n\n    select * from {{ ref('order_items') }}\n),\n\norder_summary as (\n\n    select\n        customer_id,\n\n        count(distinct orders.order_id) as count_lifetime_orders,\n        count(distinct orders.order_id) &gt; 1 as is_repeat_buyer,\n        min(orders.ordered_at) as first_ordered_at,\n        max(orders.ordered_at) as last_ordered_at,\n        sum(order_items.product_price) as lifetime_spend_pretax,\n        sum(orders.order_total) as lifetime_spend\n\n    from orders_table as orders\n\n    left join order_items_table as order_items on orders.order_id = order_items.order_id\n\n    group by 1\n\n),\n\njoined as (\n\n    select\n        customers.*,\n        order_summary.count_lifetime_orders,\n        order_summary.first_ordered_at,\n        order_summary.last_ordered_at,\n        order_summary.lifetime_spend_pretax,\n        order_summary.lifetime_spend,\n\n        case\n            when order_summary.is_repeat_buyer then 'returning'\n            else 'new'\n        end as customer_type\n\n    from customers\n\n    left join order_summary\n        on customers.customer_id = order_summary.customer_id\n\n)\n\nselect * from joined\n</code></pre> ./marts/order_items.sql <pre><code>with \n\norder_items as (\n\n    select * from {{ ref('stg_order_items') }}\n\n),\n\n\norders as (\n\n    select * from {{ ref('stg_orders')}}\n),\n\nproducts as (\n\n    select * from {{ ref('stg_products') }}\n\n),\n\nsupplies as (\n\nselect * from {{ ref('stg_supplies') }}\n\n),\n\norder_supplies_summary as (\n\nselect\n    product_id,\n    sum(supply_cost) as supply_cost\n\nfrom supplies\n\ngroup by 1\n),\n\njoined as (\n    select\n        order_items.*,\n        products.product_price,\n        order_supplies_summary.supply_cost,\n        products.is_food_item,\n        products.is_drink_item,\n        orders.ordered_at\n\n    from order_items\n\n    left join orders on order_items.order_id  = orders.order_id\n\n    left join products on order_items.product_id = products.product_id\n\n    left join order_supplies_summary on order_items.product_id = order_supplies_summary.product_id\n\n)\n\nselect * from joined\n</code></pre> ./marts/orders.sql <pre><code>with \norders as (\n\n    select * from {{ ref('stg_orders')}}\n\n),\n\norder_items_table as (\n\n    select * from {{ ref('order_items')}}\n\n),\n\norder_items_summary as (\n\n    select\n\n        order_items.order_id,\n\n        sum(supply_cost) as order_cost,\n        sum(is_food_item) as count_food_items,\n        sum(is_drink_item) as count_drink_items\n\n\n    from order_items_table as order_items\n\n    group by 1\n\n),\n\n\ncompute_booleans as (\n    select\n\n        orders.*,\n        count_food_items &gt; 0 as is_food_order,\n        count_drink_items &gt; 0 as is_drink_order,\n        order_cost\n\n    from orders\n\n    left join order_items_summary on orders.order_id = order_items_summary.order_id\n)\n\nselect * from compute_booleans\n</code></pre> </li> <li> <p>Check the lineage, even without running the models.</p> <p></p> </li> <li> <p>Run dbt build to create the data mart models in Snowflake:</p> <pre><code>dbt build\n</code></pre> <p></p> <p>Go and check the ANALYTICS database in Snowflake to see the new tables created.</p> <p></p> </li> <li> <p>Commit the changes. Use a message like <code>add dbt mart models</code>.</p> </li> </ol>"},{"location":"dbt-transformation/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>Now that the dbt transformation pipeline is configured, proceed to data quality testing.</p> <p>\ud83d\udd17 Continue to: dbt Data Quality Tests </p>"},{"location":"fivetran-setup/","title":"Fivetran Setup","text":""},{"location":"fivetran-setup/#1-create-a-fivetran-account","title":"1. Create a Fivetran Account","text":"<p>Fivetran is a cloud-based data integration platform that enables automated data ingestion from various sources.</p> <ol> <li> <p>To start using Fivetran, sign up for an account on the Fivetran website.</p> </li> <li> <p>Fill in your credentials and sign up.</p> <p></p> <p></p> </li> <li> <p>Verify your email via the confirmation link you received in your inbox - Verify my email.</p> <p></p> </li> <li> <p>After email verification, complete the form with your username and password. Also, agree to Fivetran's terms and conditions. Click Get Started.</p> <p></p> <p></p> </li> <li> <p>Skip the next step.</p> <p></p> </li> </ol>"},{"location":"fivetran-setup/#2-setup-a-google-drive-connector","title":"2. Setup a Google Drive Connector","text":"<p>Fivetran enables automated data ingestion from Google Drive.</p> <ol> <li> <p>Select Google Drive as your data source. You have to select now three connectors you are interested in, please add the Google Drive connector. For the residual two, feel free to choose any connector.</p> <p></p> <p>Click Next.</p> </li> <li> <p>Select or search for Google Drive.</p> <p></p> <p>Let's first set up our connector.</p> <p></p> </li> <li> <p>Fill out the connector requirements (find here the docs for the Google Drive connector):</p> <ul> <li>Destination schema: <code>jaffle_shop</code></li> <li> <p>Folder URL: <code>https://drive.google.com/drive/u/0/folders/1Z_U_6SVP6vWXGJWHu9Fl_uUjieyVjaWX</code></p> <p>It might ask for the following:</p> </li> <li> <p>Data processing location: <code>EU</code> </p> </li> </ul> <p>Desired state:</p> <p></p> <p>Save &amp; Test the connector.</p> </li> <li> <p>Click Save &amp; Test to verify the connection.</p> <p></p> <p>Click Continue.</p> </li> </ol> <p>Connector configuration is set up - go to the next step and add Snowflake as our destination.</p>"},{"location":"fivetran-setup/#3-configure-snowflake-as-the-destination","title":"3. Configure Snowflake as the Destination","text":"<ol> <li> <p>Select Destinations to connect Fivetran with Snowflake.</p> <p></p> <p>Add destination.</p> <p></p> <p>Select Snowflake.</p> <p></p> </li> <li> <p>Fill in the required information (Connection details from Snowflake (from your notes in <code>download.txt</code>) and additional Fivetran configuration):</p> <ul> <li>Snowflake Account URL (e.g. <code>BYELBTA-MV59868.snowflakecomputing.com</code>): <code>&lt;org_name&gt;-&lt;account_name&gt;.snowflakecomputing.com/&gt;</code></li> <li>Snowflake USER: <code>&lt;username&gt;</code></li> <li>Database: <code>PSA</code></li> <li>Auth: <code>PASSWORD</code></li> <li>Role: <code>ACCOUNTADMIN</code></li> <li>Password: <code>&lt;password&gt;</code></li> <li>Data processing location: <code>EU</code></li> <li>Fivetran processing cloud provider: <code>AWS</code></li> <li>Time Zone: <code>UTC</code></li> </ul> <p>Desired state:</p> <p></p> <ul> <li> <p>Click Save &amp; Test.</p> </li> <li> <p>All connection tests passed!</p> </li> </ul> <p></p> </li> <li> <p>View Destination and in the top right, select View for the <code>jaffle_shop</code>.</p> <p></p> <p></p> </li> </ol>"},{"location":"fivetran-setup/#4-perform-initial-data-sync","title":"4. Perform Initial Data Sync","text":"<p>Once connected, start syncing data from Google Drive to Snowflake.</p> <ol> <li> <p>Click Start Initial Sync.</p> <p></p> </li> <li> <p>Monitor progress in the Fivetran dashboard by refreshing the browser from time to time (duration ~1 min).</p> <p></p> <p>On hover over the time line block, you can see the details of the load process.</p> <p></p> </li> <li> <p>Once complete, verify that data is ingested into Snowflake under the <code>PSA</code> (Persistent Staging Area) database.</p> <p></p> </li> </ol> <p>Disable Fivetran Sync</p> <p>Feel free to turn off the sync on the top right corner. <code>ENABLED</code> should be switched to <code>PAUSED</code>.</p> <p></p> <p></p>"},{"location":"fivetran-setup/#5-what-we-have-achieved-so-far","title":"5. What we have achieved so far:","text":"<ul> <li> <p> Snowflake Ready</p> <ul> <li>\u2705 Snowflake account configured  </li> <li>\u2705 Two databases created (ANALYTICS, PSA)  </li> </ul> </li> <li> <p> dbt Cloud Connected</p> <ul> <li>\u2705 dbt Cloud set up  </li> <li>\u2705 Connected to Snowflake  </li> </ul> </li> <li> <p> Fivetran Integrated</p> <ul> <li>\u2705 Fivetran account created  </li> <li>\u2705 Connected to Snowflake  </li> </ul> </li> <li> <p> Pipeline Running</p> <ul> <li>\u2705 Fivetran syncing data from Google Drive  </li> <li>\u2705 Google Sheets loaded into jaffle_shop schema </li> </ul> </li> </ul>"},{"location":"fivetran-setup/#lets-take-a-quick-walk-through-the-ui","title":"\ud83d\ude80 Let's take a quick walk through the UI","text":""},{"location":"fivetran-setup/#next-steps","title":"\ud83c\udf89 Next Steps","text":"<p>Now that Fivetran is successfully pulling data into Snowflake, proceed to dbt transformations.</p> <p>\ud83d\udd17 Continue to: Data Integration with dbt</p>"},{"location":"resources/","title":"Resources","text":"<p>Below are key resources and references for each platform covered in the hands-on lab, along with some recommended reading and community links.</p>"},{"location":"resources/#snowflake","title":"Snowflake","text":"<ul> <li>Quickstarts</li> <li>Documentation</li> <li>YouTube: Snowflake Developers</li> </ul>"},{"location":"resources/#fivetran","title":"Fivetran","text":"<ul> <li>Fivetran Documentation</li> </ul>"},{"location":"resources/#dbt","title":"dbt","text":"<ul> <li>dbt-labs Jaffle Shop (GitHub)</li> <li>YouTube: dbt-labs (Jeremy Cohen)</li> <li>dbt Documentation</li> <li>dbt Online Training</li> <li>dbt expectations</li> </ul>"},{"location":"resources/#data-vault","title":"Data Vault","text":"<ul> <li>Hans Hultgren is a recommended trainer and expert.</li> <li>Genesis Training</li> </ul>"},{"location":"resources/#additional-resources","title":"Additional Resources","text":"<ul> <li>Medium (Patrick Cuba) </li> <li>Book: Elephant in the Fridge (highly recommended by peers)  </li> </ul>"},{"location":"resources/#support","title":"Support","text":"<p>Benjamin, you can write me any time and I will support you. Email me at mail.</p>"},{"location":"snowflake-setup/","title":"Snowflake Account Setup","text":"<p>This section walks you through setting up a Snowflake account, creating the necessary databases, and configuring the environment for the hands-on lab.</p> <p>To keep track of your login details for Snowflake, Fivetran, and dbt, download and fill out the following template:</p> <p>\ud83d\udcc4 Download Login Details Template</p> <p>This template includes sections for:</p> <ol> <li> <p>Snowflake Account Details</p> <ol> <li>Full Account Locator (e.g. <code>KG10297.AWS_EU_CENTRAL_2.AWS</code>):</li> <li>Account Locator (e.g. <code>KG10297</code>):</li> <li>Account/Server URL (e.g. <code>BYELBTA-MV59868.snowflakecomputing.com</code>): </li> <li>Username : <li>Password : <li>Warehouse Name: <code>TRANSFORMER</code></li> <li>Database Names: <code>PSA</code>, <code>ANALYTICS</code></li> <li> <p>dbt Account Details</p> <ol> <li>Username : <li>Password : <li>Project Name <code>ANALYTICS</code>:</li> <li> <p>Fivetran Account Details</p> <ol> <li>Username : <li>Password : <li>Connector Source Details:     <pre><code>https://drive.google.com/drive/u/0/folders/1Z_U_6SVP6vWXGJWHu9Fl_uUjieyVjaWX\n</code></pre></li> <p>Make sure to save this file securely and update it as needed.</p>"},{"location":"snowflake-setup/#1-creating-a-snowflake-account","title":"1. Creating a Snowflake Account","text":"<p>To get started, sign up for a free Snowflake Trial.</p> <ol> <li> <p>Visit the Snowflake Sign-Up Page. And enter your first name, last name, and work email address. Select a reason for signing up, tick if you want to opt out of marketing emails and click Continue.</p> <p></p> <p></p> </li> <li> <p>Choose your preferred cloud provider (AWS, Azure, or GCP). We prefer AWS for this lab, and also in general this is the most popular choice, since new features are usually released on AWS first. Click Get Started.</p> <p></p> <p>Captcha</p> <p>You might encounter a captcha here.</p> </li> <li> <p>You can either fill the next two forms or skip them. We recommend skipping them for now.</p> <p></p> <p></p> </li> <li> <p>Check your email and follow the activation link. It might take a few minutes for the email to arrive.</p> <p></p> <p>And here it is:</p> <p></p> </li> <li> <p>Click on the Click To Activate button. You will be redirected to the Snowflake login page.</p> <p></p> <p>Fill in a username and a password and then confirm. Note down these credentials as you will need them later.</p> <p></p> <p>Username and password requirements</p> <p>Username must be in letters and numbers. The password must contain between 14-256 characters, at least one uppercase letter, one lowercase letter and one number.</p> </li> <li> <p>You will be redirected to the Snowflake UI. You are now ready to start using Snowflake. Skip this screen for now.</p> </li> </ol>"},{"location":"snowflake-setup/#_1","title":"1. Snowflake Setup","text":""},{"location":"snowflake-setup/#2-retrieve-snowflake-account-details","title":"2. Retrieve Snowflake Account Details","text":"<p>Now you can see the standard Snowflake UI.</p> <p></p> <ol> <li> <p>Find your account Account/Server URL &amp; Full Account Locator and note them down in the <code>download.txt</code> from the top of this page. Click on your initials in the bottom left corner.</p> <p></p> <p>View account details</p> <p></p> <p>Note down the <code>Full Account Locator</code> and the <code>Account/Server URL</code></p> <p></p> </li> <li> <p>Go to the next section.</p> </li> </ol>"},{"location":"snowflake-setup/#3-creating-databases","title":"3. Creating Databases","text":"<p>For this hands-on lab, we need two databases: - PSA (Persistent Staging Area) - ANALYTICS (Final Data Warehouse)</p> UI basedCode based <ol> <li> <p>Click on the Data tab on the left.</p> <p></p> </li> <li> <p>Click Database &gt; + Database button.</p> <p></p> </li> <li> <p>Enter the database name (<code>ANALYTICS</code> and <code>PSA</code>). Then click Create.</p> <p></p> </li> <li> <p>The new databases have now appeared in the UI.</p> <p></p> </li> <li> <p>Go to the next section.</p> </li> </ol> <ol> <li> <p>Go to Projects and then Worksheets.</p> </li> <li> <p>Click on the + button on the top right corner to create a new SQL worksheet.</p> </li> <li> <p>Run the following SQL to create them:     <pre><code>CREATE DATABASE PSA;\nCREATE DATABASE ANALYTICS;\n</code></pre>     Output from PSA command:     </p> </li> <li>Verify that the databases have been created:     <pre><code>SHOW DATABASES;\n</code></pre></li> <li>You should see the two databases listed. Go to the next section.</li> </ol>"},{"location":"snowflake-setup/#4-compute-warehouse-setup","title":"4. Compute Warehouse Setup","text":"<p>Snowflake uses virtual warehouses to process queries (they are essentially workers). Create a warehouse for this lab:</p> UI basedCode based <ol> <li> <p>Click on the Admin Section tab on the left.</p> <p></p> </li> <li> <p>Select Warehouses and then click on the + Warehouse button on the top right.</p> <p></p> </li> <li> <p>Enter the warehouse name (<code>TRANSFORMER</code>), under Advanced Options add 1 for Suspend After, and click Create Warehouse.</p> <p></p> </li> <li> <p>The new warehouse has now appeared in the UI.</p> <p></p> </li> <li> <p>Go to the next section.</p> </li> </ol> <ol> <li> <p>Run the following SQL to create a warehouse named <code>TRANSFORMER</code> in a SQL worksheet:</p> <pre><code>CREATE WAREHOUSE TRANSFORMER\nWITH WAREHOUSE_SIZE = 'XSMALL'\nAUTO_SUSPEND = 60\nAUTO_RESUME = TRUE\nINITIALLY_SUSPENDED = TRUE;\n</code></pre> <p>This ensures:</p> <p>Cost Efficiency \u2192 Auto-suspends when not in use. Performance \u2192 Automatically resumes when queries are executed</p> </li> <li> <p>Verify that the warehouse has been created:</p> <pre><code>SHOW WAREHOUSES;\n</code></pre> </li> <li> <p>You should see the new warehouse listed. Go to the next section.</p> <p></p> </li> </ol>"},{"location":"snowflake-setup/#5-navigating-snowflake","title":"5. Navigating Snowflake","text":"<p>Let\u2019s explore the Snowflake UI.</p>"},{"location":"snowflake-setup/#next-steps","title":"Next Steps","text":"<p>Now that we have our user, the databases and a warehouse set up, we are ready to move on to data transformation with dbt Cloud.</p> <p>Continue to the next section: dbt Cloud Setup \ud83d\ude80</p>"}]}